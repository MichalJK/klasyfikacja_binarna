{"title":"Binary classification","markdown":{"yaml":{"title-block-banner":"#54698a","title-block-banner-color":"#dee1e3","title":"Binary classification","subtitle":"Naive Bayes and KNN","author":"Michał Kołodziejczyk","date":"now","format":{"html":{"toc":true,"toc-location":"left","tbl-pos":"H","fig-pos":"H","tbl-cap-location":"margin","fig-cap-location":"margin","code-fold":true}},"editor":"visual","jupyter":"python3","css":"styles.css"},"headingText":"The \"mtcars\" database","containsRefs":false,"markdown":"\n\nBinary classification is the classification of data into one of two categories. This assignment is the result of an algorithm that examines the structure of the database. In the case of the Naive Bayes method, the algorithm is based on Bayes' theorem of conditional probability. The K-Nearest Neighbours (KNN) algorithm classifies new data based on its similarity to data in the database. This similarity is measured by the distance between two defined categories.\n\nThe 'mtcars' database was chosen as the basis for the operation.\n\n\nThe 'mtcars' database is an R environment database. It contains data characterising the engines of the car makes listed below.\n\n```{python}\n#| label: tbl-1\n#| tbl-cap: \"Database: 'mtcars' \"\n#| message: false\n#| warning: false \n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport itables\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom itables import init_notebook_mode\nfrom itables import show  \n\ninit_notebook_mode(all_interactive=True)\n\n# Loading the base\n\nmtcars = sm.datasets.get_rdataset(\"mtcars\").data\nprint(\"\\n Databaza 'mtcart', dimensions: \",mtcars.shape[0],\" x  \",mtcars.shape[1],\" \\n\")\n\nmtcars = mtcars.dropna() \n\nshow(mtcars, layout={\"topStart\": None, \"topEnd\": None},scrollY=\"300px\", scrollCollapse=True, paging=False)\n\n```\n\n| column | description                              |\n|--------|------------------------------------------|\n| mpg    | Miles/(US) gallon                        |\n| cyl    | Number of cylinders                      |\n| disp   | Displacement (cu.in.)                    |\n| hp     | Gross horsepower                         |\n| drat   | Rear axle ratio                          |\n| wt     | Weight (lb/1000)                         |\n| qsec   | 1/4 mile time                            |\n| vs     | Engine: 0 = V-shaped , 1 = straight      |\n| am     | Transmission (0 = automatic, 1 = manual) |\n| gear   | Number of forward gears                  |\n| carb   | Number of carburetors                    |\n\n: Database 'mtcars' - variable description {#tbl-mtcars .striped .hover}\n\n# Purpose of calculations\n\n1.  Check which type of engine, i.e. 'vs' = (V-shape or straight) is better in terms of gasoline consumption (gives higher 'mpg', i.e. longer mileage per gallon of fuel)?\n\n2.  Use binary classifiers NB (simple Bayes) and KNN (k nearest neighbors) so that you can tell from 'mpg' which type of engine gave a particular 'mpg'.\n\nVariable 'vs' category: \"0\" - V-shaped ; \"1\" - straight .\n\nThe base variables 'mtcars' have been reduced to 'mpg' and 'vs'. The resulting base is called 'data'.\n\n## Practice base 'data'\n\n```{python}\n#| label: tbl-2\n#| tbl-cap: \"Practice database 'data' \"\n#| message: false\n#| warning: false\n\n# Column selection\ndata = mtcars[['mpg', 'vs']]\n\nshow(data, layout={\"topStart\": None, \"topEnd\": None},scrollY=\"300px\", scrollCollapse=True, paging=False)\n\n```\n\n80% of the database 'data' records were drawn for learning the model, while 20% were kept for testing and estimating its quality. The drawing was done without repetition.\n\n## Training data 'train'\n\n```{python}\n#| label: tbl-3\n#| tbl-cap: \"Training database 'train' \"\n#| message: false\n#| warning: false\n\nnp.random.seed(7)\n\n#Training database index selection\nindex = np.random.choice(data.index, size=int(0.8 * len(data)), replace=False)\n\n\n# Training data 'train'\ntrain = data.loc[index]\n\n# Converting 'vs' into a categorical variable\ntrain['vs'] = train['vs'].astype('category')\n\nshow(train, layout={\"topStart\": None, \"topEnd\": None},scrollY=\"300px\", scrollCollapse=True, paging=False)\n\n```\n\n## Testing data 'test'\n\n```{python}\n#| label: tbl-4\n#| tbl-cap: \"Testing database 'test' \"\n#| message: false\n#| warning: false\n\n# Test data\ntest = data.drop(index)\n\nshow(test, layout={\"topStart\": None, \"topEnd\": None}, scrollY=\"200px\", scrollCollapse=True, paging=False)\n\n```\n\n```{python}\n#| label: Wykres\n#| echo: false\n#| message: false\n#| warning: false\n\n# Charts\n\ndef wykresy(train, test, dane, probs, pp, n):\n  \n    plt.scatter(train['mpg'], train['vs'], s=3, color='blue', label='Traning data')\n    plt.scatter(test['mpg'], test['vs'], s=3, color='red', label='Testing data')\n    plt.scatter(dane['mpg'], probs[0:n,0], s = 30,  color='orange', label='Prob. vs = 0')\n    plt.scatter(dane['mpg'], probs[0:n,1], s = 30,  color='brown', label='Prob. vs = 1')\n    plt.scatter(dane['mpg'], pp, s = 100,  color='green', label='Classification results', alpha = 0.3)\n    \n    plt.legend(loc='right')\n    plt.ylabel(\"p, vs\")\n    plt.show()  \n    \n    return\n  \n```\n\n```{python}\n#| label: Wykres_ani\n#| echo: false\n#| message: false\n#| warning: false\n\nimport matplotlib.animation as animation\nfrom matplotlib.animation import FuncAnimation\n\n# Charts with animation\n\ndef wykresy_mpg_vs(train, test, dane, probs, pp, n, sss):\n\n    fig, ax = plt.subplots()\n    \n    # Animation\n    frames = 7\n    \n    def update(frames):\n        ax.scatter(dane['mpg'], pp, s=100, color='green', label='Classification results', alpha = 0.05)\n    \n    animation = FuncAnimation(fig, update,  frames=frames, interval=200)\n    \n    # Chart\n    plt.scatter(train['mpg'], train['vs'], s=3, color='blue', label='Training data')\n    plt.scatter(test['mpg'], test['vs'], s=3, color='red', label='Testing data')\n    plt.scatter(dane['mpg'], probs[0:n,0], s = 30,  color='orange', label='Prob. vs = 0')\n    plt.scatter(dane['mpg'], probs[0:n,1], s = 30,  color='brown', label='Prob. vs = 1')\n    plt.scatter(dane['mpg'], pp, s = 100,  color='green', label='Classification results', alpha = 0.2)\n    \n    plt.legend(loc='right')\n    plt.ylabel(\"p, vs\")\n    plt.xlabel(\"mpg\")\n    \n    # Save\n    animation.save(f\"{sss}{\".gif\"}\", writer=\"pillow\") \n    plt.savefig(f\"{sss}{\".png\"}\") \n    \n    plt.close()\n    \n    return\n\n```\n\n# NB - Naive Bayes\n\nA naive Bayes classifier works on a probability basis, i.e. it determines posterior probabilities and, based on these, performs a classification into one class or the other.\n\n## NB classification model and its training\n\n```{python}\n#| label: model\n#| message: false\n#| warning: false\n\n# Naive Bayes\nnb = GaussianNB()\nnb.fit(train[['mpg']], train['vs'])\n\n```\n\n## Testing the model\n\n### Classification result\n\n```{python}\n#| label: tbl-5\n#| tbl-cap: 'Results: p(“0”) - probability vs = “0”, p(“1”) - probability vs = “1”'\n#| message: false\n#| warning: false\n\n#  Prediction on test data\npredictions = nb.predict(test[['mpg']])\n\n# Posterior probability\nprobs = nb.predict_proba(test[['mpg']])  \n\n# Generating a df for test results\nwynik = pd.DataFrame({'mpg': test['mpg'], 'test[vs]': test['vs'], 'predicted_class': predictions, 'p(\"0\")':np.round(probs[0:8,0],3), 'p(\"1\")':np.round(probs[0:8,1],3)})\n\nshow(wynik, layout={\"topStart\": None, \"topEnd\": None}, scrollY=\"400px\", scrollCollapse=True, paging=False)\n\n```\n\n### Confusion matrix\n\n```{python}\n#| label: tbl-7\n#| tbl-cap: \"Confusion matrix \"\n#| message: false\n#| warning: false\n\nfrom sklearn.metrics import (\n    accuracy_score,\n    cohen_kappa_score,\n    confusion_matrix,\n    ConfusionMatrixDisplay\n)\n\ncm = confusion_matrix(test[['vs']], wynik.predicted_class )\n\ncm = pd.DataFrame(cm)\ncm.rename(columns={0:'predicted: 0',1:'predicted: 1'}, inplace=True) \ncm.rename(index={0:'actual: 0',1:'actual: 1'}, inplace=True)\nshow(cm)\n\n# disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n#                                display_labels=model.classes_)\n# disp.plot() \n\n```\n\n### Accuracy and kappa\n\n```{python}\n\naccuracy = accuracy_score(test[['vs']], wynik.predicted_class)\nprint('accuracy = ', np.round(accuracy,3))\n\nkappa = cohen_kappa_score(test[['vs']], wynik.predicted_class)\nprint('kappa = ', np.round(kappa,3))\n\n```\n\n### Charts\n\nThe @fig-10 shows the prediction results (green circle) and probabilities, serving as the basis for classification, against the training and test data.\n\n```{python}\n#| echo: false\n#| eval: false\n#| label: fig-1\n#| fig-cap: \"Summary chart of NB model testing (green circle is classification result)\"\n#| message: false\n#| warning: false\n\npp = predictions\nn = len(test)+1\n\nwykresy(train, test, test, probs, pp, n)\n\n```\n\n```{python}\n#| message: false\n#| warning: false\n\npp = predictions\nn = len(test)+1\nppp = \"fig1\"\n\nwykresy_mpg_vs(train,test,test,probs,pp,n,ppp)\n\n```\n\n![NB model testing summary chart (green flashing circles are NB classification result)](fig1.gif){#fig-10}\n\n## Prediction on the new data\n\n### New data\n\nGenerated new test data posted in @tbl-8 . The classification summary chart is shown in @fig-20 .\n\n```{python}\n#| label: tbl-8\n#| tbl-cap: \"New data \"\n#| message: false\n#| warning: false\n\n# New data for prediction\nnowe_dane = pd.DataFrame({'mpg': np.arange(10, 31, 5)})\nshow(np.transpose(nowe_dane))\n\n```\n\n### Classification\n\n```{python}\n#| label: tbl-9\n#| tbl-cap: \"Classification result for new data \"\n#| message: false\n#| warning: false\n\n# Prediction\npredykcja = nb.predict(nowe_dane[['mpg']])\nprobs = nb.predict_proba(nowe_dane[['mpg']])  # posterior\n\n\n# Generation of a df for new data\nwynik = pd.DataFrame({'mpg': nowe_dane['mpg'], 'predicted_class': predykcja, 'p(\"0\")':np.round(probs[0:5,0],3), 'p(\"1\")':np.round(probs[0:5,1],3)})\n\nshow(wynik, layout={\"topStart\": None, \"topEnd\": None}, scrollY=\"400px\", scrollCollapse=True, paging=False)\n\n```\n\n### Charts\n\n```{python}\n#| echo: false\n#| eval: false\n#| label: fig-2\n#| fig-cap: \"Summary chart for new data (green circle is the result of classification)\"\n#| message: false\n#| warning: false\n\npp_new = predykcja.astype(int)\nn = len(nowe_dane)\n\nwykresy(train, test, nowe_dane, probs, pp_new, n)\n\n```\n\n```{python}\n#| message: false\n#| warning: false\n\npp_new = predykcja.astype(int)\nn = len(nowe_dane)\n\nwykresy_mpg_vs(train,test,nowe_dane,probs,pp_new,n,\"fig2\")\n\n```\n\n![Summary chart for new data (green flashing circles are NB classification results)](fig2.gif){#fig-20}\n\n# KNN ( K Neareast Neighbor)\n\nThe same database @tbl-2 was used with the same split between the training part @tbl-3 and the test part @tbl-4 .\n\n## Training of the KNN model\n\n```{python}\n#| message: false\n#| warning: false\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Training\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(train[['mpg']], train['vs'])\n\n```\n\n## Prediction on test data\n\n### Classification results\n\n```{python}\n#| label: tbl-10\n#| tbl-cap: \"Result of KNN classification of test data\"\n#| message: false\n#| warning: false\n\n# Prediction\npredictions = knn.predict(test[['mpg']])\n\nprobs = knn.predict_proba(test[['mpg']])  \n\n# Generate test results as df\nwynik = pd.DataFrame({'mpg': test['mpg'], 'test[vs]': test['vs'], 'predicted_class': predictions, 'p(\"0\")':np.round(probs[0:8,0],3), 'p(\"1\")':np.round(probs[0:8,1],3)})\n\nshow(wynik, layout={\"topStart\": None, \"topEnd\": None}, scrollY=\"400px\", scrollCollapse=True, paging=False)\n\n\n```\n\n### Confusion matrix\n\n```{python}\n#| label: tbl-11\n#| tbl-cap: \"Confusion matrix \"\n#| message: false\n#| warning: false\n\ncm = confusion_matrix(test['vs'], predictions)\n\ncm = pd.DataFrame(cm)\ncm.rename(columns={0:'predictied: 0',1:'predicted: 1'}, inplace=True) \ncm.rename(index={0:'actual: 0',1:'actual: 1'}, inplace=True)\nshow(cm)\n\n\n```\n\n### Accuracy and kappa\n\n```{python}\n#| message: false\n#| warning: false\n\naccuracy = accuracy_score(test[['vs']], wynik.predicted_class)\nprint('accuracy = ', np.round(accuracy,3))\n\nkappa = cohen_kappa_score(test[['vs']], wynik.predicted_class)\nprint('kappa = ', np.round(kappa,3))\n\n```\n\n### Charts\n\n```{python}\n#| echo: false\n#| eval: false\n#| label: fig-3\n#| fig-cap: \"Summary chart of KNN model testing (green circle is classification result)\"\n#| message: false\n#| warning: false\n\npp = predictions\nn = len(test)+1\n\nwykresy(train, test, test, probs, pp, n)\n\n```\n\n```{python}\n#| message: false\n#| warning: false\n\npp = predictions\nn = len(test)+1\n\nwykresy_mpg_vs(train,test,test,probs,pp,n,\"fig3\")\n\n```\n\n![Summary chart of KNN model testing (green flashing circles are KNN classification results)](fig3.gif){#fig-30}\n\n## Prediction on the new data\n\n### New data\n\nWe used the test data provided in @tbl-8 . The classification summary chart is shown in @fig-40 .\n\n### Classification\n\n```{python}\n#| label: tbl-12\n#| tbl-cap: \"KNN classification result for the new data\"\n#| message: false\n#| warning: false\n\n# Prediction\npredykcja = knn.predict(nowe_dane[['mpg']])\nprobs = knn.predict_proba(nowe_dane[['mpg']])  # posterior\n\n\n# Generate a df for new data\nwynik = pd.DataFrame({'mpg': nowe_dane['mpg'], 'predicted_class': predykcja, 'p(\"0\")':np.round(probs[0:5,0],3), 'p(\"1\")':np.round(probs[0:5,1],3)})\n\nshow(wynik, layout={\"topStart\": None, \"topEnd\": None}, scrollY=\"400px\", scrollCollapse=True, paging=False)\n\n```\n\n### Charts\n\n```{python}\n#| echo: false\n#| eval: false\n#| label: fig-4\n#| fig-cap: \"Summary chart for new data (green circle is KNN classification result)\"\n#| message: false\n#| warning: false\n\npp_new = predykcja.astype(int)\nn = len(nowe_dane)\n\nwykresy(train, test, nowe_dane, probs, pp_new, n)\n\n```\n\n```{python}\n#| message: false\n#| warning: false\n\npp_new = predykcja.astype(int)\nn = len(nowe_dane)\n\nwykresy_mpg_vs(train,test,nowe_dane,probs,pp_new,n,\"fig4\")\n\n```\n\n![Summary chart for new data (green flashing circles are KNN classification results)](fig4.gif){#fig-40}\n\n# Summary\n\nGraphs @fig-10 - @fig-40 show that:\n\n-   In the mpg range from 0 to about 17: 'vs' = 0 (V-shaped),\n-   In the mpg range from about 26 to 30: 'vs' = 1 (strait).\n-   In the intermediate 'mpg' range from about 17 to 26, the probability of 'vs' being equal to 1 increases; the cutoff point is approximately at 'mpg' = 22.\n\nBoth classifiers performed similarly based on the accuracy comparison. The 'kappa' metric was slightly higher for KNN. The training base was very sparse, and most likely because of this, the results are very dependent on the draw of the training base.\n","srcMarkdownNoYaml":"\n\nBinary classification is the classification of data into one of two categories. This assignment is the result of an algorithm that examines the structure of the database. In the case of the Naive Bayes method, the algorithm is based on Bayes' theorem of conditional probability. The K-Nearest Neighbours (KNN) algorithm classifies new data based on its similarity to data in the database. This similarity is measured by the distance between two defined categories.\n\nThe 'mtcars' database was chosen as the basis for the operation.\n\n# The \"mtcars\" database\n\nThe 'mtcars' database is an R environment database. It contains data characterising the engines of the car makes listed below.\n\n```{python}\n#| label: tbl-1\n#| tbl-cap: \"Database: 'mtcars' \"\n#| message: false\n#| warning: false \n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport itables\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import train_test_split\nfrom itables import init_notebook_mode\nfrom itables import show  \n\ninit_notebook_mode(all_interactive=True)\n\n# Loading the base\n\nmtcars = sm.datasets.get_rdataset(\"mtcars\").data\nprint(\"\\n Databaza 'mtcart', dimensions: \",mtcars.shape[0],\" x  \",mtcars.shape[1],\" \\n\")\n\nmtcars = mtcars.dropna() \n\nshow(mtcars, layout={\"topStart\": None, \"topEnd\": None},scrollY=\"300px\", scrollCollapse=True, paging=False)\n\n```\n\n| column | description                              |\n|--------|------------------------------------------|\n| mpg    | Miles/(US) gallon                        |\n| cyl    | Number of cylinders                      |\n| disp   | Displacement (cu.in.)                    |\n| hp     | Gross horsepower                         |\n| drat   | Rear axle ratio                          |\n| wt     | Weight (lb/1000)                         |\n| qsec   | 1/4 mile time                            |\n| vs     | Engine: 0 = V-shaped , 1 = straight      |\n| am     | Transmission (0 = automatic, 1 = manual) |\n| gear   | Number of forward gears                  |\n| carb   | Number of carburetors                    |\n\n: Database 'mtcars' - variable description {#tbl-mtcars .striped .hover}\n\n# Purpose of calculations\n\n1.  Check which type of engine, i.e. 'vs' = (V-shape or straight) is better in terms of gasoline consumption (gives higher 'mpg', i.e. longer mileage per gallon of fuel)?\n\n2.  Use binary classifiers NB (simple Bayes) and KNN (k nearest neighbors) so that you can tell from 'mpg' which type of engine gave a particular 'mpg'.\n\nVariable 'vs' category: \"0\" - V-shaped ; \"1\" - straight .\n\nThe base variables 'mtcars' have been reduced to 'mpg' and 'vs'. The resulting base is called 'data'.\n\n## Practice base 'data'\n\n```{python}\n#| label: tbl-2\n#| tbl-cap: \"Practice database 'data' \"\n#| message: false\n#| warning: false\n\n# Column selection\ndata = mtcars[['mpg', 'vs']]\n\nshow(data, layout={\"topStart\": None, \"topEnd\": None},scrollY=\"300px\", scrollCollapse=True, paging=False)\n\n```\n\n80% of the database 'data' records were drawn for learning the model, while 20% were kept for testing and estimating its quality. The drawing was done without repetition.\n\n## Training data 'train'\n\n```{python}\n#| label: tbl-3\n#| tbl-cap: \"Training database 'train' \"\n#| message: false\n#| warning: false\n\nnp.random.seed(7)\n\n#Training database index selection\nindex = np.random.choice(data.index, size=int(0.8 * len(data)), replace=False)\n\n\n# Training data 'train'\ntrain = data.loc[index]\n\n# Converting 'vs' into a categorical variable\ntrain['vs'] = train['vs'].astype('category')\n\nshow(train, layout={\"topStart\": None, \"topEnd\": None},scrollY=\"300px\", scrollCollapse=True, paging=False)\n\n```\n\n## Testing data 'test'\n\n```{python}\n#| label: tbl-4\n#| tbl-cap: \"Testing database 'test' \"\n#| message: false\n#| warning: false\n\n# Test data\ntest = data.drop(index)\n\nshow(test, layout={\"topStart\": None, \"topEnd\": None}, scrollY=\"200px\", scrollCollapse=True, paging=False)\n\n```\n\n```{python}\n#| label: Wykres\n#| echo: false\n#| message: false\n#| warning: false\n\n# Charts\n\ndef wykresy(train, test, dane, probs, pp, n):\n  \n    plt.scatter(train['mpg'], train['vs'], s=3, color='blue', label='Traning data')\n    plt.scatter(test['mpg'], test['vs'], s=3, color='red', label='Testing data')\n    plt.scatter(dane['mpg'], probs[0:n,0], s = 30,  color='orange', label='Prob. vs = 0')\n    plt.scatter(dane['mpg'], probs[0:n,1], s = 30,  color='brown', label='Prob. vs = 1')\n    plt.scatter(dane['mpg'], pp, s = 100,  color='green', label='Classification results', alpha = 0.3)\n    \n    plt.legend(loc='right')\n    plt.ylabel(\"p, vs\")\n    plt.show()  \n    \n    return\n  \n```\n\n```{python}\n#| label: Wykres_ani\n#| echo: false\n#| message: false\n#| warning: false\n\nimport matplotlib.animation as animation\nfrom matplotlib.animation import FuncAnimation\n\n# Charts with animation\n\ndef wykresy_mpg_vs(train, test, dane, probs, pp, n, sss):\n\n    fig, ax = plt.subplots()\n    \n    # Animation\n    frames = 7\n    \n    def update(frames):\n        ax.scatter(dane['mpg'], pp, s=100, color='green', label='Classification results', alpha = 0.05)\n    \n    animation = FuncAnimation(fig, update,  frames=frames, interval=200)\n    \n    # Chart\n    plt.scatter(train['mpg'], train['vs'], s=3, color='blue', label='Training data')\n    plt.scatter(test['mpg'], test['vs'], s=3, color='red', label='Testing data')\n    plt.scatter(dane['mpg'], probs[0:n,0], s = 30,  color='orange', label='Prob. vs = 0')\n    plt.scatter(dane['mpg'], probs[0:n,1], s = 30,  color='brown', label='Prob. vs = 1')\n    plt.scatter(dane['mpg'], pp, s = 100,  color='green', label='Classification results', alpha = 0.2)\n    \n    plt.legend(loc='right')\n    plt.ylabel(\"p, vs\")\n    plt.xlabel(\"mpg\")\n    \n    # Save\n    animation.save(f\"{sss}{\".gif\"}\", writer=\"pillow\") \n    plt.savefig(f\"{sss}{\".png\"}\") \n    \n    plt.close()\n    \n    return\n\n```\n\n# NB - Naive Bayes\n\nA naive Bayes classifier works on a probability basis, i.e. it determines posterior probabilities and, based on these, performs a classification into one class or the other.\n\n## NB classification model and its training\n\n```{python}\n#| label: model\n#| message: false\n#| warning: false\n\n# Naive Bayes\nnb = GaussianNB()\nnb.fit(train[['mpg']], train['vs'])\n\n```\n\n## Testing the model\n\n### Classification result\n\n```{python}\n#| label: tbl-5\n#| tbl-cap: 'Results: p(“0”) - probability vs = “0”, p(“1”) - probability vs = “1”'\n#| message: false\n#| warning: false\n\n#  Prediction on test data\npredictions = nb.predict(test[['mpg']])\n\n# Posterior probability\nprobs = nb.predict_proba(test[['mpg']])  \n\n# Generating a df for test results\nwynik = pd.DataFrame({'mpg': test['mpg'], 'test[vs]': test['vs'], 'predicted_class': predictions, 'p(\"0\")':np.round(probs[0:8,0],3), 'p(\"1\")':np.round(probs[0:8,1],3)})\n\nshow(wynik, layout={\"topStart\": None, \"topEnd\": None}, scrollY=\"400px\", scrollCollapse=True, paging=False)\n\n```\n\n### Confusion matrix\n\n```{python}\n#| label: tbl-7\n#| tbl-cap: \"Confusion matrix \"\n#| message: false\n#| warning: false\n\nfrom sklearn.metrics import (\n    accuracy_score,\n    cohen_kappa_score,\n    confusion_matrix,\n    ConfusionMatrixDisplay\n)\n\ncm = confusion_matrix(test[['vs']], wynik.predicted_class )\n\ncm = pd.DataFrame(cm)\ncm.rename(columns={0:'predicted: 0',1:'predicted: 1'}, inplace=True) \ncm.rename(index={0:'actual: 0',1:'actual: 1'}, inplace=True)\nshow(cm)\n\n# disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n#                                display_labels=model.classes_)\n# disp.plot() \n\n```\n\n### Accuracy and kappa\n\n```{python}\n\naccuracy = accuracy_score(test[['vs']], wynik.predicted_class)\nprint('accuracy = ', np.round(accuracy,3))\n\nkappa = cohen_kappa_score(test[['vs']], wynik.predicted_class)\nprint('kappa = ', np.round(kappa,3))\n\n```\n\n### Charts\n\nThe @fig-10 shows the prediction results (green circle) and probabilities, serving as the basis for classification, against the training and test data.\n\n```{python}\n#| echo: false\n#| eval: false\n#| label: fig-1\n#| fig-cap: \"Summary chart of NB model testing (green circle is classification result)\"\n#| message: false\n#| warning: false\n\npp = predictions\nn = len(test)+1\n\nwykresy(train, test, test, probs, pp, n)\n\n```\n\n```{python}\n#| message: false\n#| warning: false\n\npp = predictions\nn = len(test)+1\nppp = \"fig1\"\n\nwykresy_mpg_vs(train,test,test,probs,pp,n,ppp)\n\n```\n\n![NB model testing summary chart (green flashing circles are NB classification result)](fig1.gif){#fig-10}\n\n## Prediction on the new data\n\n### New data\n\nGenerated new test data posted in @tbl-8 . The classification summary chart is shown in @fig-20 .\n\n```{python}\n#| label: tbl-8\n#| tbl-cap: \"New data \"\n#| message: false\n#| warning: false\n\n# New data for prediction\nnowe_dane = pd.DataFrame({'mpg': np.arange(10, 31, 5)})\nshow(np.transpose(nowe_dane))\n\n```\n\n### Classification\n\n```{python}\n#| label: tbl-9\n#| tbl-cap: \"Classification result for new data \"\n#| message: false\n#| warning: false\n\n# Prediction\npredykcja = nb.predict(nowe_dane[['mpg']])\nprobs = nb.predict_proba(nowe_dane[['mpg']])  # posterior\n\n\n# Generation of a df for new data\nwynik = pd.DataFrame({'mpg': nowe_dane['mpg'], 'predicted_class': predykcja, 'p(\"0\")':np.round(probs[0:5,0],3), 'p(\"1\")':np.round(probs[0:5,1],3)})\n\nshow(wynik, layout={\"topStart\": None, \"topEnd\": None}, scrollY=\"400px\", scrollCollapse=True, paging=False)\n\n```\n\n### Charts\n\n```{python}\n#| echo: false\n#| eval: false\n#| label: fig-2\n#| fig-cap: \"Summary chart for new data (green circle is the result of classification)\"\n#| message: false\n#| warning: false\n\npp_new = predykcja.astype(int)\nn = len(nowe_dane)\n\nwykresy(train, test, nowe_dane, probs, pp_new, n)\n\n```\n\n```{python}\n#| message: false\n#| warning: false\n\npp_new = predykcja.astype(int)\nn = len(nowe_dane)\n\nwykresy_mpg_vs(train,test,nowe_dane,probs,pp_new,n,\"fig2\")\n\n```\n\n![Summary chart for new data (green flashing circles are NB classification results)](fig2.gif){#fig-20}\n\n# KNN ( K Neareast Neighbor)\n\nThe same database @tbl-2 was used with the same split between the training part @tbl-3 and the test part @tbl-4 .\n\n## Training of the KNN model\n\n```{python}\n#| message: false\n#| warning: false\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Training\n\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(train[['mpg']], train['vs'])\n\n```\n\n## Prediction on test data\n\n### Classification results\n\n```{python}\n#| label: tbl-10\n#| tbl-cap: \"Result of KNN classification of test data\"\n#| message: false\n#| warning: false\n\n# Prediction\npredictions = knn.predict(test[['mpg']])\n\nprobs = knn.predict_proba(test[['mpg']])  \n\n# Generate test results as df\nwynik = pd.DataFrame({'mpg': test['mpg'], 'test[vs]': test['vs'], 'predicted_class': predictions, 'p(\"0\")':np.round(probs[0:8,0],3), 'p(\"1\")':np.round(probs[0:8,1],3)})\n\nshow(wynik, layout={\"topStart\": None, \"topEnd\": None}, scrollY=\"400px\", scrollCollapse=True, paging=False)\n\n\n```\n\n### Confusion matrix\n\n```{python}\n#| label: tbl-11\n#| tbl-cap: \"Confusion matrix \"\n#| message: false\n#| warning: false\n\ncm = confusion_matrix(test['vs'], predictions)\n\ncm = pd.DataFrame(cm)\ncm.rename(columns={0:'predictied: 0',1:'predicted: 1'}, inplace=True) \ncm.rename(index={0:'actual: 0',1:'actual: 1'}, inplace=True)\nshow(cm)\n\n\n```\n\n### Accuracy and kappa\n\n```{python}\n#| message: false\n#| warning: false\n\naccuracy = accuracy_score(test[['vs']], wynik.predicted_class)\nprint('accuracy = ', np.round(accuracy,3))\n\nkappa = cohen_kappa_score(test[['vs']], wynik.predicted_class)\nprint('kappa = ', np.round(kappa,3))\n\n```\n\n### Charts\n\n```{python}\n#| echo: false\n#| eval: false\n#| label: fig-3\n#| fig-cap: \"Summary chart of KNN model testing (green circle is classification result)\"\n#| message: false\n#| warning: false\n\npp = predictions\nn = len(test)+1\n\nwykresy(train, test, test, probs, pp, n)\n\n```\n\n```{python}\n#| message: false\n#| warning: false\n\npp = predictions\nn = len(test)+1\n\nwykresy_mpg_vs(train,test,test,probs,pp,n,\"fig3\")\n\n```\n\n![Summary chart of KNN model testing (green flashing circles are KNN classification results)](fig3.gif){#fig-30}\n\n## Prediction on the new data\n\n### New data\n\nWe used the test data provided in @tbl-8 . The classification summary chart is shown in @fig-40 .\n\n### Classification\n\n```{python}\n#| label: tbl-12\n#| tbl-cap: \"KNN classification result for the new data\"\n#| message: false\n#| warning: false\n\n# Prediction\npredykcja = knn.predict(nowe_dane[['mpg']])\nprobs = knn.predict_proba(nowe_dane[['mpg']])  # posterior\n\n\n# Generate a df for new data\nwynik = pd.DataFrame({'mpg': nowe_dane['mpg'], 'predicted_class': predykcja, 'p(\"0\")':np.round(probs[0:5,0],3), 'p(\"1\")':np.round(probs[0:5,1],3)})\n\nshow(wynik, layout={\"topStart\": None, \"topEnd\": None}, scrollY=\"400px\", scrollCollapse=True, paging=False)\n\n```\n\n### Charts\n\n```{python}\n#| echo: false\n#| eval: false\n#| label: fig-4\n#| fig-cap: \"Summary chart for new data (green circle is KNN classification result)\"\n#| message: false\n#| warning: false\n\npp_new = predykcja.astype(int)\nn = len(nowe_dane)\n\nwykresy(train, test, nowe_dane, probs, pp_new, n)\n\n```\n\n```{python}\n#| message: false\n#| warning: false\n\npp_new = predykcja.astype(int)\nn = len(nowe_dane)\n\nwykresy_mpg_vs(train,test,nowe_dane,probs,pp_new,n,\"fig4\")\n\n```\n\n![Summary chart for new data (green flashing circles are KNN classification results)](fig4.gif){#fig-40}\n\n# Summary\n\nGraphs @fig-10 - @fig-40 show that:\n\n-   In the mpg range from 0 to about 17: 'vs' = 0 (V-shaped),\n-   In the mpg range from about 26 to 30: 'vs' = 1 (strait).\n-   In the intermediate 'mpg' range from about 17 to 26, the probability of 'vs' being equal to 1 increases; the cutoff point is approximately at 'mpg' = 22.\n\nBoth classifiers performed similarly based on the accuracy comparison. The 'kappa' metric was slightly higher for KNN. The training base was very sparse, and most likely because of this, the results are very dependent on the draw of the training base.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":"H","fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"index_en.html"},"language":{"toc-title-document":"Spis treści","toc-title-website":"Na tej stronie","related-formats-title":"Inne formaty","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Źródło","other-links-title":"Inne odnośniki","code-links-title":"Linki do kodu","launch-dev-container-title":"Uruchom Dev Container","launch-binder-title":"Uruchom Binder","article-notebook-label":"Notatnik artykułu","notebook-preview-download":"Pobierz notatnik","notebook-preview-download-src":"Pobierz kod źródłowy","notebook-preview-back":"Powrót do artykułu","manuscript-meca-bundle":"Archiwum MECA","section-title-abstract":"Abstrakt","section-title-appendices":"Załączniki","section-title-footnotes":"Przypisy","section-title-references":"Bibliografia","section-title-reuse":"Licencja","section-title-copyright":"Prawa autorskie","section-title-citation":"Cytowanie","appendix-attribution-cite-as":"W celu atrybucji, proszę cytować tę pracę jako:","appendix-attribution-bibtex":"cytowanie BibTeX:","appendix-view-license":"Pokaż Licencję","title-block-author-single":"Autor","title-block-author-plural":"Autorzy","title-block-affiliation-single":"Afiliacja","title-block-affiliation-plural":"Afiliacje","title-block-published":"Opublikowano","title-block-modified":"Zmodyfikowano","title-block-keywords":"Słowa kluczowe","callout-tip-title":"Wskazówka","callout-note-title":"Adnotacja","callout-warning-title":"Ostrzeżenie","callout-important-title":"Ważne","callout-caution-title":"Zagrożenie","code-summary":"Kod","code-tools-menu-caption":"Kod","code-tools-show-all-code":"Pokaż cały kod","code-tools-hide-all-code":"Ukryj cały kod","code-tools-view-source":"Pokaż źródło","code-tools-source-code":"Kod źródłowy","tools-share":"Share","tools-download":"Download","code-line":"Linia","code-lines":"Linie","copy-button-tooltip":"Kopiuj do schowka","copy-button-tooltip-success":"Skopiowano!","repo-action-links-edit":"Edytuj tę stronę","repo-action-links-source":"Pokaż źródło","repo-action-links-issue":"Zgłoś problem","back-to-top":"Powrót do góry","search-no-results-text":"Brak wyników","search-matching-documents-text":"dopasowane dokumenty","search-copy-link-title":"Kopiuj link do wyszukiwania","search-hide-matches-text":"Ukryj dodatkowe dopasowania","search-more-match-text":"więcej dopasowań w tym dokumencie","search-more-matches-text":"więcej dopasowań w tym dokumencie","search-clear-button-title":"Wyczyść","search-text-placeholder":"","search-detached-cancel-button-title":"Anuluj","search-submit-button-title":"Zatwierdź","search-label":"Szukaj","toggle-section":"Przełącz sekcję","toggle-sidebar":"Przełącz pasek boczny","toggle-dark-mode":"Przełącz tryb ciemny","toggle-reader-mode":"Przełącz tryb czytnika","toggle-navigation":"Przełącz nawigację","crossref-fig-title":"Rysunek","crossref-tbl-title":"Tabela","crossref-lst-title":"Wykaz","crossref-thm-title":"Twierdzenie","crossref-lem-title":"Lemat","crossref-cor-title":"Wniosek","crossref-prp-title":"Proposition","crossref-cnj-title":"Przypuszczenie","crossref-def-title":"Definicja","crossref-exm-title":"Przykład","crossref-exr-title":"Ćwiczenie","crossref-ch-prefix":"Rozdział","crossref-apx-prefix":"Załącznik","crossref-sec-prefix":"Sekcja","crossref-eq-prefix":"Równanie","crossref-lof-title":"Spis rycin","crossref-lot-title":"Spis tabel","crossref-lol-title":"Spis wykazów","environment-proof-title":"Dowód","environment-remark-title":"Komentarz","environment-solution-title":"Rozwiązanie","listing-page-order-by":"Sortuj według","listing-page-order-by-default":"Domyślnie","listing-page-order-by-date-asc":"Od najstarszych","listing-page-order-by-date-desc":"Od najnowszych","listing-page-order-by-number-desc":"Od największych","listing-page-order-by-number-asc":"Od najmniejszych","listing-page-field-date":"Data","listing-page-field-title":"Tytuł","listing-page-field-description":"Opis","listing-page-field-author":"Autor","listing-page-field-filename":"Nazwa pliku","listing-page-field-filemodified":"Zmodyfikowano","listing-page-field-subtitle":"Podtytuł","listing-page-field-readingtime":"Czas czytania","listing-page-field-wordcount":"Licznik Słów","listing-page-field-categories":"Kategorie","listing-page-minutes-compact":"{0} min","listing-page-category-all":"Wszystkie","listing-page-no-matches":"Brak pasujących","listing-page-words":"{0} słów","listing-page-filter":"Filtr","draft":"Projekt"},"metadata":{"lang":"pl","fig-responsive":true,"quarto-version":"1.5.57","theme":"cosmo","title-block-banner":"#54698a","title-block-banner-color":"#dee1e3","title":"Binary classification","subtitle":"Naive Bayes and KNN","author":"Michał Kołodziejczyk","date":"now","editor":"visual","jupyter":"python3","toc-location":"left","tbl-pos":"H","tbl-cap-location":"margin","fig-cap-location":"margin"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}